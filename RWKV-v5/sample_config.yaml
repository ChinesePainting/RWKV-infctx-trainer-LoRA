# lightning.pytorch==2.0.5
seed_everything: true
trainer:
  target_batch_size: 32
  microbatch_size: 1
  num_nodes: 1
  devices: auto
  accelerator: gpu
  strategy: deepspeed_stage_2_offload #currently nonoffload is not supported.
  precision: bf16
  logger: null
  callbacks:
    class_path: lightning.pytorch.callbacks.ModelCheckpoint
    init_args:
      dirpath: checkpoint
      filename: test
      monitor: step
      verbose: false
      save_last: true
      save_top_k: 3
      save_weights_only: false
      mode: max
      auto_insert_metric_name: true
      every_n_train_steps: 100
      train_time_interval: null
      every_n_epochs: null
      save_on_train_epoch_end: true
  fast_dev_run: false
  max_epochs: 100
  min_epochs: null
  max_steps: -1
  min_steps: null
  max_time: null
  limit_train_batches: null
  limit_val_batches: null
  limit_test_batches: null
  limit_predict_batches: null
  overfit_batches: 0.0
  val_check_interval: null
  check_val_every_n_epoch: 1
  num_sanity_val_steps: 0
  log_every_n_steps: null
  enable_checkpointing: null
  enable_progress_bar: null
  enable_model_summary: null
  accumulate_grad_batches: 1
  gradient_clip_val: null
  gradient_clip_algorithm: null
  deterministic: null
  benchmark: null
  inference_mode: true
  use_distributed_sampler: false
  profiler: null
  detect_anomaly: false
  barebones: false
  plugins: null
  sync_batchnorm: false
  reload_dataloaders_every_n_epochs: 0
  default_root_dir: null
model:
  load_model: model/RWKV-5-World-3B-v2-20231113-ctx4096.pth
  n_embd: -1
  n_layer: -1
  lora_r: 8 #LoRA Rank
  lora_alpha: 16 #LoRA Alpha
  lora_dropout: 0.01 #LoRA Dropout
  vocab_size: -1
  ctx_len: 16384
  ctx_len_cutoffs: []
  ctx_len_warmup_steps: []
  lr_init: 0.0006
  lr_final: 0.0004
  lr_period: -1
  lr_period_type: epoch
  dropout: 0.0
  beta1: 0.9
  beta2: 0.99
  adam_eps: 1.0e-08
  weight_decay: 0.01
  warmup_steps: -1
  position_loss_bias: 1.0
  position_loss_bias_in_validation: false
  grad_cp: true
  bptt_learning: true
  bptt_learning_range: -1
  bptt_truncated_learning: false
  layerwise_lr: true
  dim_att: null
  dim_ffn: null
  substep_cuda_cache_clear: false
  substep_logging: false
  torch_set_float32_matmul_precision: high
data:
  data_path: dataset/
  source: json
  source_data_dir: dataset_src/
  source_dataset_params: null
  test_split: 0.01
  test_split_shuffle: true
  text_rechunk_size: 2048
  text_rechunk_auto: true
  text_rechunk_force: false
  tokenizer: world
  autoTokenizer: null
  world_add_endoftext_token: true
  min_token_size: 1
  max_token_size: -1
  sort_by_length: false
  sort_asc: false
  training_dataloader_shuffle_auto: true
  dataset_offset: -1.0
  dataset_length: -1.0
  custom_text_key: null
  multi_column_keys: null
  multi_column_prefix: null
  multi_column_suffix: null
  multi_column_train_mask: null
  multi_column_separator: null
  conversation_format: null
  conversation_key: null
  conversation_input_key_prefix_map: null
  conversation_input_key_mask: null
  conversation_sender_key: null
  conversation_sender_value_map: null
  conversation_input_key_map: null
  conversation_sender_suffix: null
  conversation_sender_mask: null
  conversation_end_of_conversation: null
  disable_prompt_completion_mask: false
  packing_enable: true
  packing_batchsize: 20160
  packing_chunksize: 4096
  packing_min_ctx_len: -1
  packing_in_sequence: false
  processing_max_batch_size: 100000
  skip_datapath_setup: false
ckpt_path: null
